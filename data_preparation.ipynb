{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'utils/')\n",
    "\n",
    "from dictionaries_rename import *\n",
    "from scipy.stats import rankdata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv, warnings, string\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/10017_da_en_v2_0.tab', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different features need to be processed differently\n",
    "\n",
    "We split all features into groups that need to be processed differently:\n",
    "\n",
    "A) X_personal: related to personal data (such as gender, age, etc) - what we could filter before the survey \n",
    "\n",
    "B) Related to questions about politics and their opinions:\n",
    "   * X_dummies: categorical features that need to be one hot encoded and renamed (e.g. whom did they vote);\n",
    "   * X_ordinal: ordinal features that need to be coded and renamed (e.g. strongly agree/agree/disagree/strongly disagree);\n",
    "   * X_changed_names: ordinal features that need only to be renamed (e.g. probability to vote: already from 0 to 10);\n",
    "   * X_binary: e.g. used facebook, mentioned particular question (yes/no)\n",
    "   * engineered features\n",
    "\n",
    "Predicting only 1 wave based on only previous one.\n",
    "\n",
    "Each function (almost) is executed just after it was defined so it is easier to follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## General preparation of data and functions used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "filtering only rows from particular wave, transforming dte for waves 5 and 6 into negative since survey happened after elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "waves = ['1', '2', '3', '4', '5', '6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.replace({'refused': np.NaN, '': np.NaN}, inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Making `dte` (=\"days to election\") in wave 5 and 6 negative as they took place after the elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[['w5_dte', 'w6_dte']] = df[['w5_dte', 'w6_dte']]*(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def apply_modification(function, df_dict):\n",
    "    \"\"\"makes new dataframe by implementing a function\"\"\"\n",
    "    dict_result = {}\n",
    "    for wave in waves:\n",
    "        dict_result[wave] = function(df_dict[wave])\n",
    "    return dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def select_spec_wave_respondents(df, wave):\n",
    "    \"\"\"select participants of specific wave\"\"\"\n",
    "    new_df = df[df['panelpat'].str.contains(wave)]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rename_specific_features_set(df, dict):\n",
    "    \"\"\"renaming features according to dictionaries from dictionaries_rename.py\n",
    "    and filtering these features\"\"\"\n",
    "    df_renamed = df.rename(columns=dict)\n",
    "    names = list(dict.values())\n",
    "    X_set = df_renamed[df_renamed.columns.intersection(names)]\n",
    "    return X_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "applying function to df, getting dict of dataframes for waves 1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = dict()\n",
    "for wave in waves:\n",
    "    dict_of_dfs[wave] = select_spec_wave_respondents(df, wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Filtering only wavewise questions and personal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def filter_wave_questions(wave, df):\n",
    "    \"\"\"select only questions from specific wave + personal features\"\"\"\n",
    "    \n",
    "    # wave related questions\n",
    "    df_w = df.filter(regex='w' + wave)\n",
    "    # personal features (coded with 'sd' prefix)\n",
    "    df_sd = df.filter(regex='(sd)')\n",
    "    # id, popnum and age are personal features which are not coded with prefixes\n",
    "    # 'w' or 'sd', hence picked manually\n",
    "    df_id = df['id']\n",
    "    # changing string binned value to integer \n",
    "    df.loc[df.age == '>= 70', 'age'] = 70\n",
    "    df_age = df['age']\n",
    "    y = df['panelpat']\n",
    "    df_popnum = df['popnum']\n",
    "    new_df = pd.concat([df_w, df_sd, df_popnum, df_id, df_age, y], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for wave in waves:\n",
    "    dict_of_dfs[wave] = filter_wave_questions(wave, dict_of_dfs[wave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "dropping outliers of interview duration column: who made it in less than 300 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drop_intdur_outliers(df):\n",
    "    \"\"\"drop rows with duration of the interview of less than 5 mins\"\"\"\n",
    "    intdur = df.filter(like='intdur', axis=1).columns\n",
    "    df = df[df[intdur[0]]>=300]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(drop_intdur_outliers, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Renaming only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_ordinals_to_rename(df):\n",
    "    \"\"\"renaming features of df, filtering only that are coded as numbers (e.g. probability to vote), \n",
    "    drop those which have >= 50% of NaN, replacing NaN with mean, merging with df, \n",
    "    remove initial unrenamed columns\"\"\"\n",
    "\n",
    "    X_changed_names = rename_specific_features_set(\n",
    "        df, get_ordinal_numeric_names())\n",
    "\n",
    "    # 77, 88 values are \"don't know\" and 99 is refused,\n",
    "    # so I make these values NaN for it not to be considered as ordinal values\n",
    "    X_changed_names = X_changed_names.replace(\n",
    "        dict.fromkeys([77, 88, 99], np.NaN))\n",
    "\n",
    "    X_changed_names = X_changed_names.dropna(\n",
    "        thresh=len(X_changed_names)*0.5, axis='columns')\n",
    "    X_changed_names = X_changed_names.fillna(X_changed_names.mean().round())\n",
    "    drop_list = list(get_ordinal_numeric_names().keys())\n",
    "    df = df.drop(columns=drop_list, errors='ignore')\n",
    "    df = pd.concat([df, X_changed_names], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "dict_of_dfs = apply_modification(prepare_ordinals_to_rename, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Ordinal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for those which need to be ordinally encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_ordinals_to_transform(df):\n",
    "    \"\"\"rename ordinal columns that need to be coded (e.g. opinion questions), \n",
    "    throw other features, check all the unique values of these columns, print (disabled by #),\n",
    "    replacing values with numbers, NaN values replaced by mode\"\"\"\n",
    "\n",
    "    X_ordinal = rename_specific_features_set(df, get_ordinal_str_names())\n",
    "\n",
    "    # 77, 88 values are \"don't know\" and 99 is refused,\n",
    "    # so I make these values NaN for it not to be considered as ordinal values\n",
    "\n",
    "    # replacing values depending on how many options were there\n",
    "    X_ordinal = X_ordinal.replace({\"never\": 0,\n",
    "                                   'less often': 1,\n",
    "                                   '1 day': 2,\n",
    "                                   '2 days': 3,\n",
    "                                   '3 days': 4,\n",
    "                                   '4 days': 5,\n",
    "                                   '5 days': 6,\n",
    "                                   '6 days': 7,\n",
    "                                   '7 days': 8})\n",
    "\n",
    "    X_ordinal = X_ordinal.replace({'Younger than 20': 0,\n",
    "                                   '20-29 years': 1,\n",
    "                                   '30-39 years': 2,\n",
    "                                   '40-49 years': 3,\n",
    "                                   '50-59 years': 4,\n",
    "                                   '60-69 years': 5,\n",
    "                                   'older than 69': 6})\n",
    "\n",
    "    X_ordinal = X_ordinal.replace({'never': 0,\n",
    "                                   'less often': 1,\n",
    "                                   'only on special holidays': 2,\n",
    "                                   'at least once a month': 3,\n",
    "                                   'once a week': 4,\n",
    "                                   'more often than once a week': 5,\n",
    "                                   'daily': 6})\n",
    "\n",
    "    # for questions with 5 and 6 options (sometimes it has \"don't know\" which coded the same way as neutral)\n",
    "    # so most of the features have 5 degrees coded\n",
    "    X_ordinal = X_ordinal.replace([\"not at all credible\", '2.500-', 'not well at all',\n",
    "                                   '<2.500', 'very bad decision',\n",
    "                                   'much against', 'very pessimistic', 'very bad',\n",
    "                                   'get much worse', 'got much worse',\n",
    "                                   'not proud at all', 'never', 'not at all',\n",
    "                                   'long before the election', 'not at all satisfied',\n",
    "                                   'no, definitely not', 'not at all generous',\n",
    "                                   'not at all important', 'acted completely incorrectly',\n",
    "                                   'not well at all'], 0)\n",
    "    X_ordinal = X_ordinal.replace([\"rather not credible\", '5.000-', 'fairly bad decision',\n",
    "                                   'rather against', 'rather pessimistic',\n",
    "                                   'rather bad', 'less often', 'got somewhat worse',\n",
    "                                   'get somewhat worse', 'less frequently',\n",
    "                                   'not so proud', 'rarely', 'not much',\n",
    "                                   '1 to 2 weeks before the election', 'rather not satisfied',\n",
    "                                   'no, not really', 'somewhat not generous', 'little',\n",
    "                                   'not very important', 'acted rather incorrectly',\n",
    "                                   'less well'], 1)\n",
    "    X_ordinal = X_ordinal.replace([\"partly credible/not credible\", '10.000-',\n",
    "                                   'partly good/partly bad decision',\n",
    "                                   'neither in favor nor against',\n",
    "                                   'neither optimistic nor pessimistic',\n",
    "                                   \"asylum seekers don't exist here\", 'partly good/bad',\n",
    "                                   'several times per month', 'stayed the same', 'stay the same',\n",
    "                                   'neither good nor bad', \"don't know\", 'sometimes',\n",
    "                                   'several times a month', 'partly proud', 'sometimes',\n",
    "                                   'neither generous nor not generous', \"don't know\",\n",
    "                                   'somewhat well'], 2)\n",
    "    X_ordinal = X_ordinal.replace([\"rather credible\", 'somewhat agree', '50.000-', 'fairly good decision',\n",
    "                                   'rather in favor', 'rather optimistic',\n",
    "                                   'somewhat well', 'rather good', 'several times per week',\n",
    "                                   'got somewhat better', 'get somewhat better', 'most often',\n",
    "                                   'often', 'several times a week', 'proud',\n",
    "                                   'mostly', 'rather strongly', 'few days before the election',\n",
    "                                   'rather satisfied', 'yes, somewhat', 'somewhat generous',\n",
    "                                   'rather', 'quite important', 'acted rather correctly',\n",
    "                                   'very well'], 3)\n",
    "    X_ordinal = X_ordinal.replace([\"very credible\", 'strongly agree', '1M.+', 'very good decision',\n",
    "                                   'completely agree', 'much in favor', 'very optimistic',\n",
    "                                   'very well', 'very good', 'nearly daily', 'got much better',\n",
    "                                   'get much better', 'always', 'very often',\n",
    "                                   'every day or almost every day', 'very proud', 'always',\n",
    "                                   'very strongly', 'on election day itself', 'very satisfied',\n",
    "                                   'yes, definitely', 'very generous', 'very', 'very important',\n",
    "                                   'acted completely correctly'], 4)\n",
    "    X_ordinal = X_ordinal.replace(['several times per day'], 5)\n",
    "\n",
    "    # Questions with 4 options\n",
    "    X_ordinal = X_ordinal.replace(['not at all closely', 'not at all strongly', 'not strongly at all',\n",
    "                                   'not at all important', 'did not vote', 'not at all interested',\n",
    "                                   'get along with great difficulty', 'not at all satisfied',\n",
    "                                   'very dissatisfied'], 0)\n",
    "    X_ordinal = X_ordinal.replace(['not so closely', 'less strongly', 'not very important',\n",
    "                                   'thought about it, but did not vote', 'a little interested',\n",
    "                                   'get along with difficulty', 'not very satisfied',\n",
    "                                   'fairly dissatisfied'], 1)\n",
    "    X_ordinal = X_ordinal.replace(['quite closely', 'strongly', 'quite important',\n",
    "                                   'usually vote, not this time', 'fairly interested',\n",
    "                                   'get along well', 'fairly satisfied'], 2)\n",
    "    X_ordinal = X_ordinal.replace(['very closely', 'very strongly', 'very important',\n",
    "                                   'I voted', 'very interested', 'get along very well', 'very satisfied'], 3)\n",
    "\n",
    "    X_ordinal = X_ordinal.replace({'not very close': 0,\n",
    "                                   'somewhat close': 1,\n",
    "                                   'very close': 2})\n",
    "\n",
    "    X_ordinal = X_ordinal.replace({'HIGHEST LEVEL OF EDUCATION -sd7':\n",
    "                                   {\"did not attend/finish school/elementary school or less\": 1,\n",
    "                                    \"primary school/secondary education first stage\": 2,\n",
    "                                    \"vocational training/school\": 3,\n",
    "                                    'vocational middle school': 4,\n",
    "                                    \"polytechnic institute\": 5,\n",
    "                                    \"AHS with diploma\": 6,\n",
    "                                    \"BHS with diploma\": 7,\n",
    "                                    \"BHS with diploma \": 8,\n",
    "                                    \"university-related college\": 9,\n",
    "                                    'university-related college ': 9,\n",
    "                                    \"College\": 10,\n",
    "                                    \"Bachelor\": 11,\n",
    "                                    \"Magister/Master\": 12,\n",
    "                                    \"Doctorate/PhD\": 13,\n",
    "                                    \"other\": 7}})\n",
    "\n",
    "    X_ordinal = X_ordinal.replace({'NET HOUSEHOLD INCOME -sd23':\n",
    "                                   {\"below 450 €\": 0,\n",
    "                                    \"450 up to 600 €\": 1,\n",
    "                                    \"600 to less than 750 €\": 2,\n",
    "                                    \"750 up to 900 €\": 3,\n",
    "                                    \"900 up to 1,050 €\": 4,\n",
    "                                    \"1.050 up to 1,200 €\": 5,\n",
    "                                    \"1,200 up to 1,350 €\": 6,\n",
    "                                    \"1,350 up to 1,500 €\": 7,\n",
    "                                    \"1,500 up to 1,650 €\": 8,\n",
    "                                    \"1,650 up to 1,800 €\": 9,\n",
    "                                    '1,800 up to 1,950 €': 10,\n",
    "                                    \"1,950 up to 2,100 €\": 11,\n",
    "                                    \"2,100 up to 2,250 €\": 12,\n",
    "                                    \"2,250 up to 2,400 €\": 13,\n",
    "                                    \"2,400 up to 2,700 €\": 14,\n",
    "                                    \"2,700 up to 3,000 €\": 15,\n",
    "                                    \"3,000 up to 3,300 €\": 16,\n",
    "                                    np.NaN: 16,\n",
    "                                    \"3,300 up to 3,600 €\": 17,\n",
    "                                    \"3,600 up to 3,900 €\": 18,\n",
    "                                    \"3,900 € and more\": 19}})\n",
    "\n",
    "    # Some questions are coded in a mixed way, e.g. strings for some extreme values and numbers for others\n",
    "    # For example: 'strongly dislike', 1, 2, 3, 4, 5, \"don't know person\", 6, 7, 8, 9, 'strongly like'\n",
    "    # In this subset most of the questions have 10 degrees of order. Only extreme values and \"don't know\" are encoded:\n",
    "    X_ordinal = X_ordinal.replace(['very easy access', 'strongly dislike', 'highly unlikely',\n",
    "                                   'will definitely not vote', 'left',\n",
    "                                   'do not want it at all', 'not at all competent',\n",
    "                                   'not charismatic at all', 'completely united',\n",
    "                                   'asylum laws need to be softened',\n",
    "                                   'not influenced at all', 'not fair at all',\n",
    "                                   'very bad job', 'very unsure', 'not at all willing',\n",
    "                                   \"don't trust at all\", 'do not trust at all', 'already gone too far'], 0)\n",
    "    X_ordinal = X_ordinal.replace([\"don't know\", \"don't know party\", \"don't know person\",\n",
    "                                   \"don'know\"], 5)\n",
    "    X_ordinal = X_ordinal.replace(['very restricted access', 'strongly like', 'very likely', 'highly likely',\n",
    "                                   'will definitely vote', 'right', 'want it very much',\n",
    "                                   'very competent', 'very charismatic', 'completely divided',\n",
    "                                   'asylum laws need to be toughened', 'very much',\n",
    "                                   'very strongly influenced', 'completely fair',\n",
    "                                   'very good job', 'very sure', 'not at all united',\n",
    "                                   'very willing', 'completely trust',\n",
    "                                   'should be pushed further', 'already voted by mail'], 10)\n",
    "    # UNCERTAINTY TOLERANCE\n",
    "    X_ordinal = X_ordinal.replace(['I prefer steady tasks',\n",
    "                                   'I can deal with people that are completely alien to me',\n",
    "                                   \"I prefer to know what to expect in life\",\n",
    "                                   'I dislike it when people constantly change their mind',\n",
    "                                   'I long for discussion about controversial and sensitive topics',\n",
    "                                   \"I always solve problems quickly, even if it is the first available solution\",\n",
    "                                   'I like tasks where the solution is not obvious',\n",
    "                                   'I can deal well with situations that are unpredictable'], 1)\n",
    "    X_ordinal = X_ordinal.replace(['I prefer tasks that constantly change',\n",
    "                                   \"I feel uncomfortable in the presence of people I don't know\",\n",
    "                                   'I like unexpected surprises in my life',\n",
    "                                   'I think it is alright if people change their mind',\n",
    "                                   'I tend to avoid discussion about controversial and sensitive topics',\n",
    "                                   \"I tend to ponder intensively on problems, always consulting different perspectives\",\n",
    "                                   'I prefer tasks with a clear-cut and unambiguous solution',\n",
    "                                   'I prefer to go into a situation knowing what I can expect from it'], 5)\n",
    "\n",
    "    X_ordinal = X_ordinal.replace(['often think about being Austrian', 'happy to be Austrian',\n",
    "                                   'a lot in common with other Austrians'], 1)\n",
    "    X_ordinal = X_ordinal.replace(['rarely enters my mind', 'often regret being Austrian',\n",
    "                                   'little in common with other Austrians'], 4)\n",
    "\n",
    "    # Features which contains binned str values (sd5, sd6)\n",
    "    X_ordinal.replace({'8 and more': 8, '5 and more': 5}, inplace=True)\n",
    "\n",
    "    # replacing NaN by mode\n",
    "    for column in X_ordinal.columns:\n",
    "        X_ordinal[column].fillna(X_ordinal[column].mode()[0], inplace=True)\n",
    "    # excluding string responses (like open questions)\n",
    "    # X_ordinal = X_ordinal.select_dtypes(exclude=[object])\n",
    "    drop_list = list(get_ordinal_str_names().keys())\n",
    "    df = df.drop(columns=drop_list, errors='ignore')\n",
    "    df = pd.concat([df, X_ordinal], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "dict_of_dfs = apply_modification(prepare_ordinals_to_transform, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_dummies(df):\n",
    "    \"\"\"renaming dummy features, throw others, code as dummies, throw nan columns\"\"\"\n",
    "    \n",
    "    X_dummies = rename_specific_features_set(df, get_dummies_names())\n",
    "\n",
    "    pol_cols = X_dummies.columns\n",
    "    X_dummies = pd.get_dummies(\n",
    "        X_dummies, columns=pol_cols, dummy_na=True, prefix_sep='_')\n",
    "    X_dummies = X_dummies[X_dummies.columns.drop(\n",
    "        list(X_dummies.filter(regex='nan')))]\n",
    "    drop_list = list(get_dummies_names().keys())\n",
    "    df = df.drop(columns=drop_list, errors='ignore')\n",
    "    df = pd.concat([df, X_dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(prepare_dummies, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Binary features processing (e.g. important issue: 1 if respondent mentioned the question, 0 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_binary(df):\n",
    "    \"\"\"Rename binary features, remove other columns, replacing with numbers.\"\"\"\n",
    "\n",
    "    X_binary = rename_specific_features_set(df, get_binary_names())\n",
    "\n",
    "    X_binary = X_binary.fillna(0)\n",
    "    drop_list = list(get_binary_names().keys())\n",
    "    df = df.drop(columns=drop_list, errors='ignore')\n",
    "    df = pd.concat([df, X_binary], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(prepare_binary, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Knowledge of political process\n",
    "\n",
    "whether person knows voting age (1) or not (0), whether person knows PARLIAMENTARY THRESHOLD (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def count_voting_age_awareness(df):\n",
    "    \"\"\"coded as binary feature depending on whether answer is correct\"\"\"\n",
    "    \n",
    "    age_column = df.filter(regex='w1_q31|w4f_q55|w6f_q41').columns\n",
    "    df[age_column] = df[age_column].replace([16], True)    \n",
    "    # Age with capital letter because otherwise it's mixed with personal \n",
    "    # feature of age and gets to wrong dataset of personal features\n",
    "    df = df.rename(columns={age_column[0]: 'voting_Age_awareness'})\n",
    "    # replace wrong values and NaN by 0\n",
    "    df['voting_Age_awareness'][df['voting_Age_awareness'] != True] = False\n",
    "    return df\n",
    "\n",
    "def count_parl_threshold_column(df):\n",
    "    \"\"\"coded as binary feature depending on whether answer is correct\"\"\"\n",
    "    \n",
    "    parl_threshold_column = df.filter(items=['w1_q32', 'w3_q47', 'w4f_q56', 'w6f_q42']).columns\n",
    "    df[parl_threshold_column] = df[parl_threshold_column].replace(['4%'], True)\n",
    "    df = df.rename(columns={parl_threshold_column[0]: 'knows_parl_threshold'})\n",
    "    # replace wrong values and NaN by 0\n",
    "    df['knows_parl_threshold'][df['knows_parl_threshold'] != True] = False\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# wave specific questions\n",
    "for wave in ['1', '4', '6']:\n",
    "    dict_of_dfs[wave] = count_voting_age_awareness(dict_of_dfs[wave])\n",
    "    \n",
    "for wave in ['1', '3', '4', '6']:\n",
    "    dict_of_dfs[wave] = count_parl_threshold_column(dict_of_dfs[wave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Political interest of participant\n",
    "Find out how politically interested/active a respondent is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def political_interest(df):\n",
    "    \"\"\"count sum of activities that show political interest of respondent\"\"\"\n",
    "    visited_facebook_page = df.filter(\n",
    "        like='VISITED FACEBOOK', axis=1).sum(axis=1)\n",
    "    spoke_to_party_worker = df.filter(like='TALKED', axis=1).sum(axis=1)\n",
    "    sum_interest = pd.concat(\n",
    "        [visited_facebook_page, spoke_to_party_worker], axis=1).sum(axis=1)\n",
    "    df['political_interest'] = sum_interest\n",
    "    #sum_interest = pd.DataFrame(sum_interest)\n",
    "    #sum_interest.columns = ['political_interest']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(political_interest, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Correct left-right identification\n",
    "Did participant correctly place the given parties on a politic (left-right) spectrum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* ÖVP - right\n",
    "* GRÜNE - left\n",
    "* SPÖ - left\n",
    "* FPÖ - right\n",
    "* NEOS - centrism (excluded)\n",
    "* TEAM STRONACH - right\n",
    "* LIST PETER PILZ - left\n",
    "\n",
    "Initial data are from 0 (left) to 10 (right). If a person assigns 0:3 to a right party or 7:10 to a left party or 0:2/8:10 to centrism party then it is considered as wrong answer and coded as 0. Otherwise it is 1. New variable **lr_placement_correct**: rate how many times person placed a party in a \"correct\" way described above\n",
    "\n",
    "**correct_placement_bin**: bottom 25% from lr_placement_correct are 0, others are 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def correct_placement(df):\n",
    "    \"\"\"get binary feature with 25% of people who were the most incorrect and others\"\"\"\n",
    "    lr_placement = df.filter(like='LEFT-RIGHT PLACEMENT:', axis=1)\n",
    "    rights = lr_placement.filter(regex='OEVP|FPOE|TEAM STRONACH')\n",
    "    rights.replace([0, 1, 2, 3], 0, inplace=True)\n",
    "    rights[rights != 0] = 1\n",
    "    lefts = lr_placement.filter(regex='THE GREENS|SPOE|LIST PETER PILZ')\n",
    "    lefts.replace([7, 8, 9, 10], 0, inplace=True)\n",
    "    lefts[lefts != 0] = 1\n",
    "\n",
    "    lr_placement_bin = pd.concat([rights, lefts], axis=1)\n",
    "    lr_placement_correct = lr_placement_bin.sum(\n",
    "        axis=1) / lr_placement_bin.shape[1]\n",
    "    lr_placement_correct = pd.DataFrame(lr_placement_correct)\n",
    "    lr_placement_correct.columns = ['lr_placement_correct']\n",
    "    lr_placement_correct.fillna(0, inplace=True)\n",
    "    df['lr_placement_correct'] = lr_placement_correct\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(correct_placement, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**ONLY W1 and W2:** making CHECK QUESTIONs binary (1 for correct answer, 0 for incorrect answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_check_question(df):\n",
    "    \"\"\"coded as binary feature depending on whether answer is correct\"\"\"\n",
    "    check_question = df.loc[:, df.columns.isin(['w1_q27x5', 'w2_q24x5'])]\n",
    "    check_question = check_question.replace(['somewhat disagree'], 1)\n",
    "    check_question[check_question != 1] = 0\n",
    "    check_question = pd.DataFrame(check_question)\n",
    "    check_question.columns = ['check_question']\n",
    "    return check_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "check_question_w1 = prepare_check_question(dict_of_dfs['1'])\n",
    "check_question_w2 = prepare_check_question(dict_of_dfs['2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "counting number of words in open questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def open_q_number(df):\n",
    "    \"\"\"count sum of words in open questions\"\"\"\n",
    "    df_open_q = df.filter(regex='w2_q51x5t|w4_q84x5t')\n",
    "    df_open_q.columns = ['words_open_question']\n",
    "    df_open_q['words_open_question'] = df_open_q['words_open_question'].str.split(\n",
    "    ).str.len()\n",
    "    df_open_q.fillna(0, inplace=True)\n",
    "    return df_open_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_open_q_w2 = open_q_number(dict_of_dfs['2'])\n",
    "df_open_q_w4 = open_q_number(dict_of_dfs['4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Counting answers ratio\n",
    "counting answers ratio:\n",
    "* \"don't know\"\n",
    "* 'other'\n",
    "* 'other party'\n",
    "* 77 = do not know party/person\n",
    "* 88 = don't know\n",
    "\n",
    "Total 5 features (I did not concatenated \"don't know\" responses (77, 88) in case there are some patterns depending on type of answer (numeric or word) - so 3 \"don't know\" features are considered...\n",
    "\n",
    "Total number of features with mentioned above responses (which I divide by when count ratio) is counted as maximum of all such responses in all rows. E.g. if nobody responded \"don't know\" in some question then the feature was not counted. This is because I don't know number of questions with \"don't know\" as possible answer. Also because for example 'other' is coded differently in different questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def count_bad_quality_ratios(df):\n",
    "    \"\"\"count sum of [\"don't know\", 'other', 'other party', 77, 88] responces, \n",
    "    make new features with number of it\"\"\"\n",
    "    bad_quality = [\"don't know\", 'other', 'other party', 77, 88]\n",
    "    bad_quality_str = [\"don't know\", 'other', 'other party', '77', '88']\n",
    "\n",
    "    for i in bad_quality:\n",
    "        df[str(i)+\"-bad_quality\"] = (df == i).sum(axis=1)\n",
    "        df[str(i)+\"-bad_quality\"] = df[str(i)+'-bad_quality'] / \\\n",
    "            df[df.columns[df.isin([i]).any()]].shape[1]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "dict_of_dfs = apply_modification(count_bad_quality_ratios, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Number of \"don't knows\"\n",
    "Compute mean value in bad quality columns, threshold for bad quality is 3rd quartile (bad is coded as 0), else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dont_know_percentage(df):\n",
    "    \"\"\"count mean of [\"don't know\", 'other', 'other party', 77, 88] columns\"\"\"\n",
    "    X = df[[\"don't know-bad_quality\",  '77-bad_quality', '88-bad_quality']]\n",
    "    df['dont_know_percentage_mean'] = X.sum(axis=1) / X.shape[1]\n",
    "    return df\n",
    "\n",
    "\n",
    "dict_of_dfs = apply_modification(dont_know_percentage, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Correct left-right identification of politicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def know_politicians_ratio(df):\n",
    "    \"\"\"rate of how well they know politicians\"\"\"\n",
    "    X_know_politicians_ratio = df.loc[:, df.columns.isin(['KNOWLEDGE: HANS-PETER DOSKOZIL -w1_q33x1_SPOE',\n",
    "                                                                        'KNOWLEDGE: SOPHIE KARMASIN -w1_q33x2_OEVP',  # not sure\n",
    "                                                                        'KNOWLEDGE: SONJA HAMMERSCHMID -w1_q33x3_SPOE',\n",
    "                                                                        'KNOWLEDGE: HERBERT KICKL -w1_q33x4_FPOE',\n",
    "                                                                        'KNOWLEDGE: HANS-PETER DOSKOZIL -w4f_q57x1_SPOE',\n",
    "                                                                        'KNOWLEDGE: SOPHIE KARMASIN -w4f_q57x2_OEVP',\n",
    "                                                                        'KNOWLEDGE: SONJA HAMMERSCHMID -w4f_q57x3_SPOE',\n",
    "                                                                        'KNOWLEDGE: HERBERT KICKL -w4f_q57x4_FPOE',\n",
    "                                                                        'KNOWLEDGE: HANS-PETER DOSKOZIL -w6f_q43x1_SPOE',\n",
    "                                                                        'KNOWLEDGE: SOPHIE KARMASIN -w6f_q43x2_OEVP',\n",
    "                                                                        'KNOWLEDGE: SONJA HAMMERSCHMID -w6f_q43x3_SPOE',\n",
    "                                                                        'KNOWLEDGE: HERBERT KICKL -w6f_q43x4_FPOE'])].sum(axis=1) / 4\n",
    "\n",
    "    df['know_politicians_ratio'] = X_know_politicians_ratio\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(\n",
    "    know_politicians_ratio, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_label(df):\n",
    "    \"\"\"add 'OPINION: ' label to opinion related questions \n",
    "    (e.g. strongly agree/completely agree) for easier filtering \"\"\"\n",
    "    \n",
    "    opinion_cols = df.filter(regex='|'.join(opinion_questions), axis=1).columns\n",
    "    for col in opinion_cols:\n",
    "        df.rename(columns={col: 'OPINION: ' + col}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(add_label, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# agree/disagree with some opinion\n",
    "def same_response_rate(df):\n",
    "    \"\"\"check for straighlining (rate of opinion questions answered with the same option)\"\"\"\n",
    "\n",
    "    X_mode_agreement = df.filter(like='OPINION', axis=1)\n",
    "    same_agree_resp_rate = X_mode_agreement.stack().groupby(\n",
    "        level=0).value_counts().max(level=0) / X_mode_agreement.shape[1]\n",
    "    df['same_agree_resp'] = same_agree_resp_rate\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(same_response_rate, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### (In)consistent answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We filter out a few very similar (or diametrically opposed) questions and check if a participant gave similar (or opposed) answers\n",
    "\n",
    "Manually, we selected the following questions.\n",
    "* PREFER INDEPENDENT CITIZEN INSTEAD OF A PARTY MEMBER -w1_q27x8\t\n",
    "* THE PEOPLE SHOULD TAKE MOST IMPORTANT DECISIONS, NOT POLITICIANS -w1_q27x7\n",
    "* 'FEELING LIKE A STRANGER DUE TO THE MANY MUSLIMS -w2_q21x4',\n",
    "* 'EUROPEAN AND MUSLIM LIFESTYLE ARE EASILY COMPATIBLE -w2_q21x5',\n",
    "* PEOPLE LIKE ME HAVE RECEIVED LESS THAN THEY DESERVE -w3_q35x1\t\n",
    "* PEOPLE LIKE ME GET LESS ATTENTION THAN OTHERS -w3_q35x2\n",
    "* 'SAME ACCESS TO SOCIAL BENEFITS: ASYLUM SEEKERS -w4_q65x2',\n",
    "* 'SAME ACCESS TO SOCIAL BENEFITS: NON-AUSTRIANS -w4_q65x1',\n",
    "* IMMIGRANTS GET MORE ATTENTION -w5_q30x2\t\n",
    "* IMMIGRANTS HAVE RECEIVED MORE THAN THEY DESERVE -w5_q30x1\n",
    "* OPINION: MOST POLITICIANS ARE TRUSTWORTHY -w6_q34x3,\n",
    "* OPINION: POLITICIANS DO NOT CARE ABOUT WHAT PEOPLE LIKE ME THINK -w6_q34x5\n",
    "\n",
    "This feature will be __$1$ if an answer is inconsistent__\n",
    "and $0$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def consistency_check(df):\n",
    "    \"\"\"Filter some similar (or diametrically opposed) questions \n",
    "    and check if a participant gave similar (or opposed) answeres\"\"\"\n",
    "\n",
    "    positive_corr_opinion = df.loc[:, df.columns.isin(\n",
    "        ['OPINION: PREFER INDEPENDENT CITIZEN INSTEAD OF A PARTY MEMBER -w1_q27x8',\n",
    "         'OPINION: THE PEOPLE SHOULD TAKE MOST IMPORTANT DECISIONS, NOT POLITICIANS -w1_q27x7',\n",
    "         'OPINION: PEOPLE LIKE ME HAVE RECEIVED LESS THAN THEY DESERVE -w3_q35x1',\n",
    "         'OPINION: PEOPLE LIKE ME GET LESS ATTENTION THAN OTHERS -w3_q35x2',\n",
    "         'OPINION: IMMIGRANTS GET MORE ATTENTION -w5_q30x2',\n",
    "         'OPINION: IMMIGRANTS HAVE RECEIVED MORE THAN THEY DESERVE -w5_q30x1',\n",
    "         'OPINION: SAME ACCESS TO SOCIAL BENEFITS: ASYLUM SEEKERS -w4_q65x2',\n",
    "         'OPINION: SAME ACCESS TO SOCIAL BENEFITS: NON-AUSTRIANS -w4_q65x1',\n",
    "         'OPINION: FEELING LIKE A STRANGER DUE TO THE MANY MUSLIMS -w2_q21x4',\n",
    "         'OPINION: EUROPEAN AND MUSLIM LIFESTYLE ARE EASILY COMPATIBLE -w2_q21x5',\n",
    "         'OPINION: MOST POLITICIANS ARE TRUSTWORTHY -w6_q34x3',\n",
    "         'OPINION: POLITICIANS DO NOT CARE ABOUT WHAT PEOPLE LIKE ME THINK -w6_q34x5'])]\n",
    "    positive_corr_opinion.replace([0, 1], 0, inplace=True)  # disagree\n",
    "    positive_corr_opinion.replace([4, 5], 1, inplace=True)  # agree\n",
    "    positive_corr_opinion.replace([2, 3], np.NaN, inplace=True)\n",
    "    inconsistency = positive_corr_opinion[positive_corr_opinion.columns[0]].eq(\n",
    "        positive_corr_opinion[positive_corr_opinion.columns[1]]).astype(int)\n",
    "    inconsistency = abs(inconsistency-1)\n",
    "\n",
    "    df['inconsistency'] = inconsistency\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(consistency_check, dict_of_dfs)\n",
    "# in waves 2 and 6 questions are quite opposite in terms of inconsistency\n",
    "dict_of_dfs['2']['inconsistency'] = abs(dict_of_dfs['2']['inconsistency']-1)\n",
    "dict_of_dfs['6']['inconsistency'] = abs(dict_of_dfs['6']['inconsistency']-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def response_hour(df):\n",
    "    \"\"\"function returns columns of which time of the day the response \n",
    "    was given and binary feature of day of the week (weekday/weekend)\"\"\"\n",
    "    timestamp = df.filter(like='_date', axis=1).astype('datetime64[ns]')\n",
    "    timestamp.columns = ['timestamp_colname']\n",
    "    hour = timestamp.timestamp_colname.dt.hour\n",
    "    hour = pd.DataFrame(hour)\n",
    "    hour.columns = ['timeOfResponding']\n",
    "    nighttime = list([23, 0, 1, 2, 3, 4, 5])\n",
    "    morningtime = list(range(6, 9))\n",
    "    worktime = list(range(9, 17))\n",
    "    eveningtime = list(range(17, 23))\n",
    "\n",
    "    hour.replace(nighttime, 'nighttime', inplace=True)\n",
    "    hour.replace(morningtime, 'morning', inplace=True)\n",
    "    hour.replace(worktime, '9-5', inplace=True)\n",
    "    hour.replace(eveningtime, 'evening', inplace=True)\n",
    "    hour_dummies = pd.get_dummies(hour, dummy_na=True, prefix_sep='_')\n",
    "\n",
    "    timestamp['weekendResponse'] = timestamp['timestamp_colname'].dt.day_name()\n",
    "    timestamp['weekendResponse'].replace(\n",
    "        ['Saturday', 'Sunday'], 1, inplace=True)\n",
    "    timestamp['weekendResponse'][timestamp['weekendResponse'] != 1] = 0\n",
    "    time_vars = pd.concat([timestamp['weekendResponse'], hour_dummies], axis=1)\n",
    "    df = pd.concat([df, time_vars], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(response_hour, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Days to respond (`dte`)\n",
    "We noticed that the `dte` (=\"days to election\") feature show up significantly in the model predicting attrition. For easier interpretation we \"normalized\" this feature counting how many days after the first respondent a participant completed the wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def days_to_respond(df):\n",
    "    \"\"\"how many days it took to respond assuming all the samples got the survey at the same time,\n",
    "    starting point was from the very first response registered\"\"\"\n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "    dtes = (df.filter(like='dte', axis=1)).to_numpy()\n",
    "    rankdata_list = rankdata(list(map(lambda x: -x, dtes)), method='dense')\n",
    "    rankdata_df = pd.DataFrame(rankdata_list)\n",
    "    rankdata_df.columns = ['days_to_respond']\n",
    "    rankdata_df.replace(list(range(9, 17)), 9, inplace=True)\n",
    "    rankdata_df.index = df.index\n",
    "    df['days_to_respond'] = rankdata_df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = apply_modification(days_to_respond, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### Participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def participated_only_once(df):\n",
    "    \"\"\"binary feature: checking if person participated only once \n",
    "    (since there are only 3 values we filter that manually)\"\"\"\n",
    "    \n",
    "    df['participated_only_once'] = df['panelpat'].isin(['1.....', '.....6', '...4..'])\n",
    "    return df\n",
    "\n",
    "dict_of_dfs = apply_modification(participated_only_once, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def refreshment_respondent(df):\n",
    "    \"\"\"Feature indicating whether person joined survey after wave 3 (code: True) or not (code: False)\"\"\"\n",
    "    \n",
    "    df['refreshment'] = ~df['panelpat'].str.contains('1|2|3')\n",
    "    return df\n",
    "\n",
    "dict_of_dfs = apply_modification(refreshment_respondent, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def came_back(wave, df):\n",
    "    \"\"\"if participant has dropped the survey and then came back in a later wave, \n",
    "    the value equals to the number of the last wave one has participated before the current one, \n",
    "    otherwise if one has never dropped or never came back: 0\"\"\"\n",
    "    if wave == '1' or wave == '2':\n",
    "        return df\n",
    "    else:\n",
    "        panelpat_df = pd.DataFrame(df['panelpat'])\n",
    "        panelpat_df['panelpat'] = panelpat_df['panelpat'].str.replace('.', '0')\n",
    "        panelpat_df[['previous_waves', 'future_waves']] = panelpat_df['panelpat'].str.split(\n",
    "            '0' + str(wave), expand=True)\n",
    "        # participant can miss 2 waves in a row (or more), therefore we delete duplicate 0 to get the latest wave of participation\n",
    "        panelpat_df.previous_waves = panelpat_df.previous_waves.apply(\n",
    "            lambda w: \"\".join(sorted(set(w))))\n",
    "        df['came_back_from'] = [w.strip()[-1]\n",
    "                                for w in panelpat_df['previous_waves']]\n",
    "        # if one has not dropped in previous wave we also change is to 0\n",
    "        df.loc[df['came_back_from'] >= wave, 'came_back_from'] = 0\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for wave in waves:\n",
    "    dict_of_dfs[wave] = came_back(wave, dict_of_dfs[wave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Did they drop before?\n",
    "New variable: whether participant dropped the survey at least in one of previous wave:\n",
    "        * 1 if dropped before\n",
    "        * 0 if participated in all waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# respondents recrouted later are considered as those who never dropped \n",
    "# ('...456': 186 samples and '.....6': 117 samples)\n",
    "def dropped_before(wave, df):\n",
    "    \"\"\"split panelpat into 2 parts, number of wave is delimeter, \n",
    "    get previous and future waves of participation (in terms of current wave)\n",
    "    then check if there are missings in previous waves,\n",
    "    if so, then coded as 1 (dropped before), otherwise 0\"\"\"\n",
    "    panelpat_df = pd.DataFrame(df['panelpat'])\n",
    "    panelpat_df[panelpat_df == '...456|.....6'] = 0\n",
    "    panelpat_df[['previous_waves', 'future_waves']\n",
    "                ] = panelpat_df['panelpat'].str.split(wave, expand=True)\n",
    "    whether_dropped_before = panelpat_df['previous_waves']\n",
    "    whether_dropped_before = whether_dropped_before.str.contains(\n",
    "        '.', regex=False)\n",
    "    df['whether_dropped_before'] = whether_dropped_before\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for wave in waves:\n",
    "    dict_of_dfs[wave] = dropped_before(wave, dict_of_dfs[wave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* transforming 'panelpat' into classes: \n",
    "    * 0 for people who dropped\n",
    "    * 1 for respondents who stayed\n",
    "* making y series (which is panelpat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transform_panelpat(wave, df):\n",
    "    \"\"\"coding panelpat as follows: if person participated in next wave, then: 1, otherwise: 0, \n",
    "    extract dependent variable for concatenating at the end with all the prepared features\"\"\"\n",
    "    i = int(wave)\n",
    "    i += 1\n",
    "    i = str(i)\n",
    "    df['panelpat'] = df['panelpat'].str.contains(i).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for wave in ['1', '2', '3', '4', '5']:\n",
    "    dict_of_dfs[wave] = transform_panelpat(wave, dict_of_dfs[wave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Splitting into political and personal datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "concatenating wave specific columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def list_for_splitting(data_all):\n",
    "    \"\"\"used to split df into political and personal features\"\"\"\n",
    "    ohe_features = ['HOUSEHOLD SIZE -sd5',\n",
    "                    'MEMBERS OF HOUSEHOLD YOUNGER THAN 18 YEARS -sd6',\n",
    "                    'HIGHEST LEVEL OF EDUCATION -sd7',\n",
    "                    'ATTENDANCE OF RELIGIOUS SERVICES -sd9',\n",
    "                    'JOB SITUATION -sd11',\n",
    "                    'INCOME SITUATION -sd22',\n",
    "                    'NET HOUSEHOLD INCOME -sd23',\n",
    "                    'DESCRIPTION OF RESIDENTIAL AREA -sd24',\n",
    "                    'ADDITIONAL OCCUPATION -sd13',\n",
    "                    'RELIGIOUS AFFILIATION -sd8',\n",
    "                    'Country of birth, repondent -sd18',\n",
    "                    'Country of birth, mother -sd19',\n",
    "                    'Country of birth, father -sd20',\n",
    "                    'GENDER -sd3',\n",
    "                    'age',\n",
    "                    'popnum',\n",
    "                    'FEDERAL STATE -sd4',\n",
    "                    'CURRENT PERSONAL SITUATION -sd10',\n",
    "                    'OTHER OCCUPATION -sd12',\n",
    "                    'TYPE OF OCCUPATION -sd14',\n",
    "                    'PREVIOUS TYPE OF OCCUPATION -sd16',\n",
    "                    'AUSTRIAN CITIZENSHIP FROM BIRTH -sd17',\n",
    "                    'UNION MEMBERSHIP -sd21',\n",
    "                    'EVER EMPLOYMENT -sd15',\n",
    "                    'came_back_from']\n",
    "    transformed_features_names = []\n",
    "    for i in ohe_features:\n",
    "        a = list(data_all.filter(like=i, axis=1))\n",
    "        transformed_features_names.append(a)\n",
    "\n",
    "    list_personals = []\n",
    "    for i in list(range(0, len(transformed_features_names))):\n",
    "        for j in list(range(0, len(transformed_features_names[i]))):\n",
    "            b = transformed_features_names[i][j]\n",
    "            list_personals.append(b)\n",
    "    return list_personals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs['1'] = pd.concat([dict_of_dfs['1'],\n",
    "                              check_question_w1], axis=1)\n",
    "\n",
    "dict_of_dfs['2'] = pd.concat([dict_of_dfs['2'],\n",
    "                              check_question_w2,\n",
    "                              df_open_q_w2], axis=1)\n",
    "\n",
    "dict_of_dfs['4'] = pd.concat([dict_of_dfs['4'],\n",
    "                              df_open_q_w4,\n",
    "                              ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_personal = {}\n",
    "X_political = {}\n",
    "for wave in waves:\n",
    "    list_personals = list_for_splitting(dict_of_dfs[wave])\n",
    "    X_personal[wave] = dict_of_dfs[wave][list_personals]\n",
    "    X_political[wave] = dict_of_dfs[wave].drop(list_personals, axis=1)\n",
    "    X_political[wave]['dont_know_percentage_mean'] = X_personal[wave]['dont_know_percentage_mean']\n",
    "    X_personal[wave] = X_personal[wave].drop(['dont_know_percentage_mean'], axis=1)\n",
    "    # some id values are missing but only 1 per wave, so we impute index there\n",
    "    X_political[wave].interpolate(inplace=True)\n",
    "    # drop redundant columns (such as open questions because we \n",
    "    # already added its prepared versions), also\n",
    "    # columns with NaN and others like those which we did not use (e.g. w1_panelist)\n",
    "    redundant_columns = ['w1_panelist', 'w1_weightd', 'w1_weightp', 'w1_date', 'sd1',\n",
    "                         'w2_panelist', 'w2_weightd', 'w2_weightp', 'w2_date', 'w2_q45x1t_id',\n",
    "                         'w2_q45x2t_id', 'w3_panelist', 'w3_weightd', 'w3_weightp', 'w3_date',\n",
    "                         'w4_panelist', 'w4_weightd', 'w4_weightp', 'w4_date', 'w4f_q56t', 'w4_q62t',\n",
    "                         'w4_q63t', 'w4_q64t', 'w4_q78x1t_id', 'w4_q78x2t_id', 'w4_q80x5t', 'w4_q84x5t',\n",
    "                         'w5_panelist', 'w5_weightd', 'w5_weightp', 'w5_date',\n",
    "                         'w6_panelist', 'w6_weightd', 'w6_weightp', 'w6_date', 'w6_exp1',\n",
    "                         'w6_q31t', 'w6_q31dur', 'w6_q32t', 'w6_q33t', 'w6_q46x1t_id',\n",
    "                         'w6_q46x2t_id', 'w6_q57split', 'w6f_q42t']\n",
    "    X_political[wave] = X_political[wave].drop(\n",
    "        columns=redundant_columns, errors='ignore')\n",
    "    X_political[wave].dropna(axis=1, inplace=True)\n",
    "    # filling age NaN by mean\n",
    "    X_personal[wave] = X_personal[wave].fillna(\n",
    "        value=X_personal[wave]['age'].mean())\n",
    "# making dependent feature (panelpat) right-hand sided\n",
    "waves = ['1', '2', '3', '4', '5']\n",
    "for wave in waves:\n",
    "    X_political[wave]['panelpat'] = X_political[wave].pop('panelpat')\n",
    "    X_personal[wave]['panelpat'] = X_political[wave]['panelpat']\n",
    "    X_personal[wave]['id'] = X_political[wave]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Some final checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "checking why for some datasets number of columns is different from previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave:  1\n",
      "X_personal:  (4019, 103)\n",
      "X_political:  (4019, 356)\n",
      "Wave:  2\n",
      "X_personal:  (3133, 103)\n",
      "X_political:  (3133, 337)\n",
      "Wave:  3\n",
      "X_personal:  (2994, 104)\n",
      "X_political:  (2994, 445)\n",
      "Wave:  4\n",
      "X_personal:  (3166, 104)\n",
      "X_political:  (3166, 621)\n",
      "Wave:  5\n",
      "X_personal:  (3026, 104)\n",
      "X_political:  (3026, 353)\n",
      "Wave:  6\n",
      "X_personal:  (2974, 102)\n",
      "X_political:  (2974, 373)\n"
     ]
    }
   ],
   "source": [
    "waves = ['1', '2', '3', '4', '5', '6']\n",
    "for wave in waves:\n",
    "    print('Wave: ', wave)\n",
    "    print('X_personal: ', X_personal[wave].shape)\n",
    "    print('X_political: ', X_political[wave].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "check for NaN in all datasets and getting its index if relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave:  1\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n",
      "Wave:  2\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n",
      "Wave:  3\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n",
      "Wave:  4\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n",
      "Wave:  5\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n",
      "Wave:  6\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for wave in waves:\n",
    "    print('Wave: ', wave)\n",
    "    print(X_personal[wave].isnull().values.any())\n",
    "    print(X_personal[wave].loc[pd.isnull(\n",
    "        X_personal[wave]).any(1), :].index.values)\n",
    "    print(X_political[wave].isnull().values.any())\n",
    "    print(X_political[wave].loc[pd.isnull(\n",
    "        X_political[wave]).any(1), :].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wave in waves:\n",
    "    X_political[wave].to_csv(\n",
    "        'data/data_online_political_w' + wave + '.csv', index=False)\n",
    "    X_personal[wave].to_csv(\n",
    "        'data/data_online_personal_w' + wave + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
