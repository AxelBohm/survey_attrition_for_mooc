{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import dictionaries_rename as naming\n",
    "from utils import more_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/10017_da_en_v2_0.tab', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Different features need to be processed differently\n",
    "\n",
    "We split all features into groups that need to be processed differently:\n",
    "\n",
    "A) X_personal: related to personal data (such as gender, age, etc) - what we could filter before the survey \n",
    "\n",
    "B) Related to questions about politics and their opinions:\n",
    "   * X_dummies: categorical features that need to be one hot encoded and renamed (e.g. whom did they vote);\n",
    "   * X_ordinal: ordinal features that need to be coded and renamed (e.g. strongly agree/agree/disagree/strongly disagree);\n",
    "   * X_changed_names: ordinal features that need only to be renamed (e.g. probability to vote: already from 0 to 10);\n",
    "   * X_binary: e.g. used facebook, mentioned particular question (yes/no)\n",
    "   * engineered features\n",
    "\n",
    "Predicting only 1 wave based on only previous one.\n",
    "\n",
    "Each function (almost) is executed just after it was defined so it is easier to follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General preparation of data and functions used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering only rows from particular wave, transforming dte for waves 5 and 6 into negative since survey happened after elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "WAVENOS = ['1', '2', '3', '4', '5', '6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'refused': np.NaN, '': np.NaN}, inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making `dte` (=\"days to election\") in wave 5 and 6 negative as they took place after the elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['w5_dte', 'w6_dte']] = -df[['w5_dte', 'w6_dte']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valmap(function, df_dict):\n",
    "    \"\"\"Applies function to all values of dictionary.\"\"\"\n",
    "    dict_result = {}\n",
    "    for wave in WAVENOS:\n",
    "        dict_result[wave] = function(df_dict[wave])\n",
    "    return dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_spec_wave_respondents(df, wave):\n",
    "    \"\"\"select participants of specific wave\"\"\"\n",
    "    new_df = df[df['panelpat'].str.contains(wave)]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_specific_features_set(df, cols):\n",
    "    \"\"\"renaming features according to dictionaries from dictionaries_rename.py\n",
    "    and filtering these features\"\"\"\n",
    "    df_renamed = df.rename(columns=cols)\n",
    "    names = list(cols.values())\n",
    "    X_set = df_renamed[df_renamed.columns.intersection(names)]\n",
    "    return X_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying function to df, getting dict of dataframes for waves 1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = dict()\n",
    "for wave in WAVENOS:\n",
    "    dict_of_dfs[wave] = select_spec_wave_respondents(df, wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering only wavewise questions and personal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns_for_wave(df, waveno):\n",
    "    \"\"\"Select only questions from specific wave + personal features.\"\"\"\n",
    "    \n",
    "    wave_related_questions = more_utils.find_cols(df, regex=f'w{waveno}')\n",
    "    personal_features = more_utils.find_cols(df, regex='(sd)')\n",
    "    additional_pers_cols = ['popnum', 'id', 'age', 'panelpat']\n",
    "    relevant_cols = wave_related_questions + personal_features + additional_pers_cols\n",
    "\n",
    "    return df[relevant_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wave in WAVENOS:\n",
    "    dict_of_dfs[wave] = select_columns_for_wave(dict_of_dfs[wave], wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropping outliers of interview duration column: who made it in less than 300 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_intdur_outliers(df):\n",
    "    \"\"\"drop rows with duration of the interview of less than 5 mins\"\"\"\n",
    "    intdur = df.filter(like='intdur', axis=1).columns\n",
    "    df = df[df[intdur[0]]>=300]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(drop_intdur_outliers, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of *don't know* answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave this function here because it needs original values preserved (although logically belongs to **engineered features** part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dont_know_ratio(df):\n",
    "    \"\"\"77, 88 values are either don't know/don't know party/don't know the person\n",
    "    :output: the df with ratio column of such responses\"\"\"\n",
    "    df['dont_know_ratio'] = ((df == 77).sum(axis=1) + (df == 88).sum(axis=1))/df.shape[1]\n",
    "    return df\n",
    "\n",
    "dict_of_dfs = valmap(dont_know_ratio, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for those which need to be ordinally encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ordinals_to_transform(df):\n",
    "    \"\"\"rename ordinal columns that need to be coded (e.g. opinion questions), \n",
    "    throw other features, check all the unique values of these columns, print (disabled by #),\n",
    "    replacing values with numbers, NaN values replaced by mode\"\"\"\n",
    "\n",
    "    X_ordinal = rename_specific_features_set(df, naming.get_ordinal_names())\n",
    "    \n",
    "    # 77, 88 values are \"don't know\" and 99 is refused, 12 is \"would vote invalid\"\n",
    "    # so we make these values NaN for it not to be considered as ordinal values\n",
    "    X_ordinal = X_ordinal.replace(dict.fromkeys([77, 88, 99, 12], np.NaN))\n",
    "\n",
    "    # replacing NaN by mode\n",
    "    for column in X_ordinal.columns:\n",
    "        X_ordinal[column].fillna(X_ordinal[column].mode()[0], inplace=True)\n",
    "    # excluding string responses (like open questions)\n",
    "    # X_ordinal = X_ordinal.select_dtypes(exclude=[object])\n",
    "    drop_list = list(naming.get_ordinal_names().keys())\n",
    "    df = df.drop(columns=drop_list, errors='ignore')\n",
    "    df = pd.concat([df, X_ordinal], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "dict_of_dfs = valmap(prepare_ordinals_to_transform, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dummies(df):\n",
    "    \"\"\"renaming dummy features, throw others, code as dummies, throw nan columns\"\"\"\n",
    "    \n",
    "    X_dummies = rename_specific_features_set(df, naming.get_dummies_names())\n",
    "\n",
    "    pol_cols = X_dummies.columns\n",
    "    X_dummies = pd.get_dummies(\n",
    "        X_dummies, columns=pol_cols, dummy_na=True, prefix_sep='__')\n",
    "    X_dummies = X_dummies[X_dummies.columns.drop(\n",
    "        list(X_dummies.filter(regex='nan')))]\n",
    "    drop_list = list(naming.get_dummies_names().keys())\n",
    "    df = df.drop(columns=drop_list, errors='ignore')\n",
    "    df = pd.concat([df, X_dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(prepare_dummies, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary features processing (e.g. important issue: 1 if respondent mentioned the question, 0 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_binary(df):\n",
    "    \"\"\"Rename binary features, remove other columns, replacing with numbers.\"\"\"\n",
    "\n",
    "    X_binary = rename_specific_features_set(df, naming.get_binary_names())\n",
    "\n",
    "    X_binary = X_binary.fillna(0)\n",
    "    drop_list = list(naming.get_binary_names().keys())\n",
    "    df = df.drop(columns=drop_list, errors='ignore')\n",
    "    df = pd.concat([df, X_binary], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(prepare_binary, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge of political process\n",
    "\n",
    "whether person knows voting age (1) or not (0), whether person knows PARLIAMENTARY THRESHOLD (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_voting_age_awareness(df):\n",
    "    \"\"\"coded as binary feature depending on whether answer is correct\"\"\"\n",
    "    \n",
    "    age_column = df.filter(regex='w1_q31|w4f_q55|w6f_q41').columns\n",
    "    df[age_column] = df[age_column].replace([16], True)    \n",
    "    # Age with capital letter because otherwise it's mixed with personal \n",
    "    # feature of age and gets to wrong dataset of personal features\n",
    "    df = df.rename(columns={age_column[0]: 'voting_Age_awareness'})\n",
    "    # replace wrong values and NaN by 0\n",
    "    df['voting_Age_awareness'][df['voting_Age_awareness'] != True] = False\n",
    "    return df\n",
    "\n",
    "def count_parl_threshold_column(df):\n",
    "    \"\"\"coded as binary feature depending on whether answer is correct\"\"\"\n",
    "    \n",
    "    parl_threshold_column = df.filter(items=['w1_q32', 'w3_q47', 'w4f_q56', 'w6f_q42']).columns\n",
    "    df[parl_threshold_column] = df[parl_threshold_column].replace(['4%'], 1)\n",
    "    df = df.rename(columns={parl_threshold_column[0]: 'knows_parl_threshold'})\n",
    "    # replace wrong values and NaN by 0\n",
    "    df['knows_parl_threshold'][df['knows_parl_threshold'] != 1] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wave specific questions\n",
    "for wave in ['1', '4', '6']:\n",
    "    dict_of_dfs[wave] = count_voting_age_awareness(dict_of_dfs[wave])\n",
    "    \n",
    "for wave in ['1', '3', '4', '6']:\n",
    "    dict_of_dfs[wave] = count_parl_threshold_column(dict_of_dfs[wave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Political interest of participant\n",
    "Find out how politically interested/active a respondent is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def political_interest(df):\n",
    "    \"\"\"count sum of activities that show political interest of respondent\"\"\"\n",
    "    visited_facebook_page = df.filter(\n",
    "        like='VISITED FACEBOOK', axis=1).sum(axis=1)\n",
    "    spoke_to_party_worker = df.filter(like='TALKED', axis=1).sum(axis=1)\n",
    "    sum_interest = pd.concat(\n",
    "        [visited_facebook_page, spoke_to_party_worker], axis=1).sum(axis=1)\n",
    "    df['political_interest'] = sum_interest\n",
    "    #sum_interest = pd.DataFrame(sum_interest)\n",
    "    #sum_interest.columns = ['political_interest']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(political_interest, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct left-right identification\n",
    "Did participant correctly place the given parties on a politic (left-right) spectrum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ÖVP - right\n",
    "* GRÜNE - left\n",
    "* SPÖ - left\n",
    "* FPÖ - right\n",
    "* NEOS - centrism (excluded)\n",
    "* TEAM STRONACH - right\n",
    "* LIST PETER PILZ - left\n",
    "\n",
    "Initial data are from 0 (left) to 10 (right). If a person assigns 0:3 to a right party or 7:10 to a left party or 0:2/8:10 to centrism party then it is considered as wrong answer and coded as 0. Otherwise it is 1. New variable **lr_placement_correct**: rate how many times person placed a party in a \"correct\" way described above\n",
    "\n",
    "**correct_placement_bin**: bottom 25% from lr_placement_correct are 0, others are 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_placement(df):\n",
    "    \"\"\"get binary feature with 25% of people who were the most incorrect and others\"\"\"\n",
    "    lr_placement = df.filter(like='LEFT-RIGHT PLACEMENT:', axis=1)\n",
    "    rights = lr_placement.filter(regex='OEVP|FPOE|TEAM STRONACH')\n",
    "    rights.replace([0, 1, 2, 3], 0, inplace=True)\n",
    "    rights[rights != 0] = 1\n",
    "    lefts = lr_placement.filter(regex='THE GREENS|SPOE|LIST PETER PILZ')\n",
    "    lefts.replace([7, 8, 9, 10], 0, inplace=True)\n",
    "    lefts[lefts != 0] = 1\n",
    "\n",
    "    lr_placement_bin = pd.concat([rights, lefts], axis=1)\n",
    "    lr_placement_correct = lr_placement_bin.sum(\n",
    "        axis=1) / lr_placement_bin.shape[1]\n",
    "    lr_placement_correct = pd.DataFrame(lr_placement_correct)\n",
    "    lr_placement_correct.columns = ['lr_placement_correct']\n",
    "    lr_placement_correct.fillna(0, inplace=True)\n",
    "    df['lr_placement_correct'] = lr_placement_correct\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(correct_placement, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ONLY W1 and W2:** making CHECK QUESTIONs binary (1 for correct answer, 0 for incorrect answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_check_question(df):\n",
    "    \"\"\"coded as binary feature depending on whether answer is correct\"\"\"\n",
    "    check_question = df.loc[:, df.columns.isin(['w1_q27x5', 'w2_q24x5'])]\n",
    "    check_question[check_question != 4] = 0 # 4 is 'somewhat disagree'\n",
    "    check_question = check_question.replace(4, 1)\n",
    "    check_question = pd.DataFrame(check_question)\n",
    "    check_question.columns = ['check_question']\n",
    "    return check_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_question_w1 = prepare_check_question(dict_of_dfs['1'])\n",
    "check_question_w2 = prepare_check_question(dict_of_dfs['2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counting number of words in open questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def open_q_number(df):\n",
    "    \"\"\"count sum of words in open questions\"\"\"\n",
    "    df_open_q = df.filter(regex='w2_q51x5t|w4_q84x5t')\n",
    "    df_open_q.columns = ['words_open_question']\n",
    "    df_open_q['words_open_question'] = df_open_q['words_open_question'].str.split(\n",
    "    ).str.len()\n",
    "    df_open_q.fillna(0, inplace=True)\n",
    "    return df_open_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_open_q_w2 = open_q_number(dict_of_dfs['2'])\n",
    "df_open_q_w4 = open_q_number(dict_of_dfs['4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct left-right identification of politicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def know_politicians_ratio(df):\n",
    "    \"\"\"rate of how well they know politicians\"\"\"\n",
    "    X_know_politicians_ratio = df.loc[:, df.columns.isin(['KNOWLEDGE: HANS-PETER DOSKOZIL -w1_q33x1_1.0',\n",
    "                                                                        'KNOWLEDGE: SOPHIE KARMASIN -w1_q33x2_2.0',  # not sure\n",
    "                                                                        'KNOWLEDGE: SONJA HAMMERSCHMID -w1_q33x3_1.0',\n",
    "                                                                        'KNOWLEDGE: HERBERT KICKL -w1_q33x4_3.0',\n",
    "                                                                        'KNOWLEDGE: HANS-PETER DOSKOZIL -w4f_q57x1_1.0',\n",
    "                                                                        'KNOWLEDGE: SOPHIE KARMASIN -w4f_q57x2_2.0',\n",
    "                                                                        'KNOWLEDGE: SONJA HAMMERSCHMID -w4f_q57x3_1.0',\n",
    "                                                                        'KNOWLEDGE: HERBERT KICKL -w4f_q57x4_3.0',\n",
    "                                                                        'KNOWLEDGE: HANS-PETER DOSKOZIL -w6f_q43x1_1.0',\n",
    "                                                                        'KNOWLEDGE: SOPHIE KARMASIN -w6f_q43x2_2.0',\n",
    "                                                                        'KNOWLEDGE: SONJA HAMMERSCHMID -w6f_q43x3_1.0',\n",
    "                                                                        'KNOWLEDGE: HERBERT KICKL -w6f_q43x4_3.0'])].sum(axis=1) / 4\n",
    "\n",
    "    df['know_politicians_ratio'] = X_know_politicians_ratio\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(know_politicians_ratio, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(df):\n",
    "    \"\"\"add 'OPINION: ' label to opinion related questions \n",
    "    (e.g. strongly agree/completely agree) for easier filtering \"\"\"\n",
    "    \n",
    "    opinion_cols = df.filter(regex='|'.join(naming.opinion_questions), axis=1).columns\n",
    "    for col in opinion_cols:\n",
    "        df.rename(columns={col: 'OPINION: ' + col}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(add_label, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agree/disagree with some opinion\n",
    "def same_response_rate(df):\n",
    "    \"\"\"check for straighlining (rate of opinion questions answered with the same option)\"\"\"\n",
    "\n",
    "    X_mode_agreement = df.filter(like='OPINION', axis=1)\n",
    "    same_agree_resp_rate = X_mode_agreement.stack().groupby(\n",
    "        level=0).value_counts().max(level=0) / X_mode_agreement.shape[1]\n",
    "    df['same_agree_resp'] = same_agree_resp_rate\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(same_response_rate, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (In)consistent answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter out a few very similar (or diametrically opposed) questions and check if a participant gave similar (or opposed) answers\n",
    "\n",
    "Manually, we selected the following questions.\n",
    "* PREFER INDEPENDENT CITIZEN INSTEAD OF A PARTY MEMBER -w1_q27x8\t\n",
    "* THE PEOPLE SHOULD TAKE MOST IMPORTANT DECISIONS, NOT POLITICIANS -w1_q27x7\n",
    "* 'FEELING LIKE A STRANGER DUE TO THE MANY MUSLIMS -w2_q21x4',\n",
    "* 'EUROPEAN AND MUSLIM LIFESTYLE ARE EASILY COMPATIBLE -w2_q21x5',\n",
    "* PEOPLE LIKE ME HAVE RECEIVED LESS THAN THEY DESERVE -w3_q35x1\t\n",
    "* PEOPLE LIKE ME GET LESS ATTENTION THAN OTHERS -w3_q35x2\n",
    "* 'SAME ACCESS TO SOCIAL BENEFITS: ASYLUM SEEKERS -w4_q65x2',\n",
    "* 'SAME ACCESS TO SOCIAL BENEFITS: NON-AUSTRIANS -w4_q65x1',\n",
    "* IMMIGRANTS GET MORE ATTENTION -w5_q30x2\t\n",
    "* IMMIGRANTS HAVE RECEIVED MORE THAN THEY DESERVE -w5_q30x1\n",
    "* OPINION: MOST POLITICIANS ARE TRUSTWORTHY -w6_q34x3,\n",
    "* OPINION: POLITICIANS DO NOT CARE ABOUT WHAT PEOPLE LIKE ME THINK -w6_q34x5\n",
    "\n",
    "This feature will be __$1$ if an answer is inconsistent__\n",
    "and $0$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def consistency_check(df):\n",
    "    \"\"\"Filter some similar (or diametrically opposed) questions \n",
    "    and check if a participant gave similar (or opposed) answeres\"\"\"\n",
    "\n",
    "    positive_corr_opinion = df.loc[:, df.columns.isin(\n",
    "        ['OPINION: PREFER INDEPENDENT CITIZEN INSTEAD OF A PARTY MEMBER -w1_q27x8',\n",
    "         'OPINION: THE PEOPLE SHOULD TAKE MOST IMPORTANT DECISIONS, NOT POLITICIANS -w1_q27x7',\n",
    "         'OPINION: PEOPLE LIKE ME HAVE RECEIVED LESS THAN THEY DESERVE -w3_q35x1',\n",
    "         'OPINION: PEOPLE LIKE ME GET LESS ATTENTION THAN OTHERS -w3_q35x2',\n",
    "         'OPINION: IMMIGRANTS GET MORE ATTENTION -w5_q30x2',\n",
    "         'OPINION: IMMIGRANTS HAVE RECEIVED MORE THAN THEY DESERVE -w5_q30x1',\n",
    "         'OPINION: SAME ACCESS TO SOCIAL BENEFITS: ASYLUM SEEKERS -w4_q65x2',\n",
    "         'OPINION: SAME ACCESS TO SOCIAL BENEFITS: NON-AUSTRIANS -w4_q65x1',\n",
    "         'OPINION: FEELING LIKE A STRANGER DUE TO THE MANY MUSLIMS -w2_q21x4',\n",
    "         'OPINION: EUROPEAN AND MUSLIM LIFESTYLE ARE EASILY COMPATIBLE -w2_q21x5',\n",
    "         'OPINION: MOST POLITICIANS ARE TRUSTWORTHY -w6_q34x3',\n",
    "         'OPINION: POLITICIANS DO NOT CARE ABOUT WHAT PEOPLE LIKE ME THINK -w6_q34x5'])]\n",
    "    positive_corr_opinion.replace([0, 1], 0, inplace=True)  # disagree\n",
    "    positive_corr_opinion.replace([4, 5], 1, inplace=True)  # agree\n",
    "    positive_corr_opinion.replace([2, 3], np.NaN, inplace=True)\n",
    "    inconsistency = positive_corr_opinion[positive_corr_opinion.columns[0]].eq(\n",
    "        positive_corr_opinion[positive_corr_opinion.columns[1]]).astype(int)\n",
    "    inconsistency = abs(inconsistency-1)\n",
    "\n",
    "    df['inconsistency'] = inconsistency\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(consistency_check, dict_of_dfs)\n",
    "# in waves 2 and 6 questions are quite opposite in terms of inconsistency\n",
    "dict_of_dfs['2']['inconsistency'] = abs(dict_of_dfs['2']['inconsistency']-1)\n",
    "dict_of_dfs['6']['inconsistency'] = abs(dict_of_dfs['6']['inconsistency']-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_hour(df):\n",
    "    \"\"\"function returns columns of which time of the day the response \n",
    "    was given and binary feature of day of the week (weekday/weekend)\"\"\"\n",
    "    timestamp = df.filter(like='_date', axis=1).astype('datetime64[ns]')\n",
    "    timestamp.columns = ['timestamp_colname']\n",
    "    hour = timestamp.timestamp_colname.dt.hour\n",
    "    hour = pd.DataFrame(hour)\n",
    "    hour.columns = ['timeOfResponding']\n",
    "    nighttime = list([23, 0, 1, 2, 3, 4, 5])\n",
    "    morningtime = list(range(6, 9))\n",
    "    worktime = list(range(9, 17))\n",
    "    eveningtime = list(range(17, 23))\n",
    "\n",
    "    hour.replace(nighttime, 'nighttime', inplace=True)\n",
    "    hour.replace(morningtime, 'morning', inplace=True)\n",
    "    hour.replace(worktime, '9-5', inplace=True)\n",
    "    hour.replace(eveningtime, 'evening', inplace=True)\n",
    "    hour_dummies = pd.get_dummies(hour, dummy_na=True, prefix_sep='_')\n",
    "\n",
    "    timestamp['weekendResponse'] = timestamp['timestamp_colname'].dt.day_name()\n",
    "    timestamp['weekendResponse'].replace(\n",
    "        ['Saturday', 'Sunday'], 1, inplace=True)\n",
    "    timestamp['weekendResponse'][timestamp['weekendResponse'] != 1] = 0\n",
    "    time_vars = pd.concat([timestamp['weekendResponse'], hour_dummies], axis=1)\n",
    "    df = pd.concat([df, time_vars], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(response_hour, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days to respond (`dte`)\n",
    "We noticed that the `dte` (=\"days to election\") feature show up significantly in the model predicting attrition. For easier interpretation we \"normalized\" this feature counting how many days after the first respondent a participant completed the wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_to_respond(df):\n",
    "    \"\"\"How many days it took to respond, assuming all the samples got the survey at the same time.\n",
    "    Starting point was from the very first response registered.\"\"\"\n",
    "    dtes = (df.filter(like='dte', axis=1))\n",
    "    days_to_respond = (dtes - dtes.max()).abs()\n",
    "    days_to_respond.clip(0, 8, inplace=True) # lump the higher ones togeter\n",
    "    df['days_to_respond'] = days_to_respond\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs = valmap(days_to_respond, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def participated_only_once(df):\n",
    "    \"\"\"binary feature: checking if person participated only once \n",
    "    (since there are only 3 values we filter that manually)\"\"\"\n",
    "    \n",
    "    df['participated_only_once'] = df['panelpat'].isin(['1.....', '.....6', '...4..'])\n",
    "    return df\n",
    "\n",
    "dict_of_dfs = valmap(participated_only_once, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refreshment_respondent(df):\n",
    "    \"\"\"Feature indicating whether person joined survey after wave 3 (code: True) or not (code: False)\"\"\"\n",
    "    \n",
    "    df['refreshment'] = ~df['panelpat'].str.contains('1|2|3')\n",
    "    return df\n",
    "\n",
    "dict_of_dfs = valmap(refreshment_respondent, dict_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def came_back(wave, df):\n",
    "    \"\"\"if participant has dropped the survey and then came back in a later wave, \n",
    "    the value equals to the number of the last wave one has participated before the current one, \n",
    "    otherwise if one has never dropped or never came back: 0\"\"\"\n",
    "    if wave == '1' or wave == '2':\n",
    "        return df\n",
    "    else:\n",
    "        panelpat_df = pd.DataFrame(df['panelpat'])\n",
    "        panelpat_df['panelpat'] = panelpat_df['panelpat'].str.replace('.', '0')\n",
    "        panelpat_df[['previous_waves', 'future_waves']] = panelpat_df['panelpat'].str.split(\n",
    "            '0' + str(wave), expand=True)\n",
    "        # participant can miss 2 waves in a row (or more), therefore we delete duplicate 0 to get the latest wave of participation\n",
    "        panelpat_df.previous_waves = panelpat_df.previous_waves.apply(\n",
    "            lambda w: \"\".join(sorted(set(w))))\n",
    "        df['came_back_from'] = [w.strip()[-1]\n",
    "                                for w in panelpat_df['previous_waves']]\n",
    "        # if one has not dropped in previous wave we also change is to 0\n",
    "        df.loc[df['came_back_from'] >= wave, 'came_back_from'] = 0\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for wave in waves:\n",
    "#    dict_of_dfs[wave] = came_back(wave, dict_of_dfs[wave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did they drop before?\n",
    "New variable: whether participant dropped the survey at least in one of previous wave:\n",
    "        * 1 if dropped before\n",
    "        * 0 if participated in all waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# respondents recrouted later are considered as those who never dropped \n",
    "# ('...456': 186 samples and '.....6': 117 samples)\n",
    "def dropped_before(wave, df):\n",
    "    \"\"\"split panelpat into 2 parts, number of wave is delimeter, \n",
    "    get previous and future waves of participation (in terms of current wave)\n",
    "    then check if there are missings in previous waves,\n",
    "    if so, then coded as 1 (dropped before), otherwise 0\"\"\"\n",
    "    panelpat_df = pd.DataFrame(df['panelpat'])\n",
    "    panelpat_df[panelpat_df == '...456|.....6'] = 0\n",
    "    panelpat_df[['previous_waves', 'future_waves']\n",
    "                ] = panelpat_df['panelpat'].str.split(wave, expand=True)\n",
    "    whether_dropped_before = panelpat_df['previous_waves']\n",
    "    whether_dropped_before = whether_dropped_before.str.contains(\n",
    "        '.', regex=False)\n",
    "    df['whether_dropped_before'] = whether_dropped_before\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wave in WAVENOS:\n",
    "    dict_of_dfs[wave] = dropped_before(wave, dict_of_dfs[wave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transforming 'panelpat' into classes: \n",
    "    * 0 for people who dropped\n",
    "    * 1 for respondents who stayed\n",
    "* making y series (which is panelpat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_panelpat(wave, df):\n",
    "    \"\"\"coding panelpat as follows: if person participated in next wave, then: 1, otherwise: 0, \n",
    "    extract dependent variable for concatenating at the end with all the prepared features\"\"\"\n",
    "    i = int(wave)\n",
    "    i += 1\n",
    "    i = str(i)\n",
    "    df['panelpat'] = df['panelpat'].str.contains(i).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wave in ['1', '2', '3', '4', '5']:\n",
    "    dict_of_dfs[wave] = transform_panelpat(wave, dict_of_dfs[wave])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into political and personal datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenating wave specific columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_for_splitting(data_all):\n",
    "    \"\"\"used to split df into political and personal features\"\"\"\n",
    "    ohe_features = ['HOUSEHOLD SIZE -sd5',\n",
    "                    'MEMBERS OF HOUSEHOLD YOUNGER THAN 18 YEARS -sd6',\n",
    "                    'HIGHEST LEVEL OF EDUCATION -sd7',\n",
    "                    'ATTENDANCE OF RELIGIOUS SERVICES -sd9',\n",
    "                    'JOB SITUATION -sd11',\n",
    "                    'INCOME SITUATION -sd22',\n",
    "                    'NET HOUSEHOLD INCOME -sd23',\n",
    "                    'DESCRIPTION OF RESIDENTIAL AREA -sd24',\n",
    "                    'ADDITIONAL OCCUPATION -sd13',\n",
    "                    'RELIGIOUS AFFILIATION -sd8',\n",
    "                    'Country of birth, repondent -sd18',\n",
    "                    'Country of birth, mother -sd19',\n",
    "                    'Country of birth, father -sd20',\n",
    "                    'GENDER -sd3',\n",
    "                    'age',\n",
    "                    'popnum',\n",
    "                    'FEDERAL STATE -sd4',\n",
    "                    'CURRENT PERSONAL SITUATION -sd10',\n",
    "                    'OTHER OCCUPATION -sd12',\n",
    "                    'TYPE OF OCCUPATION -sd14',\n",
    "                    'PREVIOUS TYPE OF OCCUPATION -sd16',\n",
    "                    'AUSTRIAN CITIZENSHIP FROM BIRTH -sd17',\n",
    "                    'UNION MEMBERSHIP -sd21',\n",
    "                    'EVER EMPLOYMENT -sd15',\n",
    "                    'came_back_from']\n",
    "    transformed_features_names = []\n",
    "    for i in ohe_features:\n",
    "        a = list(data_all.filter(like=i, axis=1))\n",
    "        transformed_features_names.append(a)\n",
    "\n",
    "    list_personals = []\n",
    "    for i in list(range(0, len(transformed_features_names))):\n",
    "        for j in list(range(0, len(transformed_features_names[i]))):\n",
    "            b = transformed_features_names[i][j]\n",
    "            list_personals.append(b)\n",
    "    return list_personals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_dfs['1'] = pd.concat([dict_of_dfs['1'],\n",
    "                              check_question_w1], axis=1)\n",
    "\n",
    "dict_of_dfs['2'] = pd.concat([dict_of_dfs['2'],\n",
    "                              check_question_w2,\n",
    "                              df_open_q_w2], axis=1)\n",
    "\n",
    "dict_of_dfs['4'] = pd.concat([dict_of_dfs['4'],\n",
    "                              df_open_q_w4,\n",
    "                              ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_personal = {}\n",
    "X_political = {}\n",
    "for wave in WAVENOS:\n",
    "    list_personals = list_for_splitting(dict_of_dfs[wave])\n",
    "    X_personal[wave] = dict_of_dfs[wave][list_personals]\n",
    "    X_political[wave] = dict_of_dfs[wave].drop(list_personals, axis=1)\n",
    "    # some id values are missing but only 1 per wave, so we impute index there\n",
    "    X_political[wave].interpolate(inplace=True)\n",
    "    # drop redundant columns (such as open questions because we \n",
    "    # already added its prepared versions), also\n",
    "    # columns with NaN and others like those which we did not use (e.g. w1_panelist)\n",
    "    redundant_columns = ['w1_panelist', 'w1_weightd', 'w1_weightp', 'w1_date', 'sd1',\n",
    "                         'w2_panelist', 'w2_weightd', 'w2_weightp', 'w2_date', 'w2_q45x1t_id',\n",
    "                         'w2_q45x2t_id', 'w3_panelist', 'w3_weightd', 'w3_weightp', 'w3_date',\n",
    "                         'w4_panelist', 'w4_weightd', 'w4_weightp', 'w4_date', 'w4f_q56t', 'w4_q62t',\n",
    "                         'w4_q63t', 'w4_q64t', 'w4_q78x1t_id', 'w4_q78x2t_id', 'w4_q80x5t', 'w4_q84x5t',\n",
    "                         'w5_panelist', 'w5_weightd', 'w5_weightp', 'w5_date',\n",
    "                         'w6_panelist', 'w6_weightd', 'w6_weightp', 'w6_date', 'w6_exp1',\n",
    "                         'w6_q31t', 'w6_q31dur', 'w6_q32t', 'w6_q33t', 'w6_q46x1t_id',\n",
    "                         'w6_q46x2t_id', 'w6_q57split', 'w6f_q42t']\n",
    "    X_political[wave] = X_political[wave].drop(\n",
    "        columns=redundant_columns, errors='ignore')\n",
    "    X_political[wave].dropna(axis=1, inplace=True)\n",
    "    # filling age NaN by mean\n",
    "    X_personal[wave] = X_personal[wave].fillna(\n",
    "        value=X_personal[wave]['age'].mean())\n",
    "# making dependent feature (panelpat) right-hand sided\n",
    "for wave in ['1', '2', '3', '4', '5']:\n",
    "    X_political[wave]['panelpat'] = X_political[wave].pop('panelpat')\n",
    "    X_personal[wave]['panelpat'] = X_political[wave]['panelpat']\n",
    "    X_personal[wave]['id'] = X_political[wave]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some final checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking why for some datasets number of columns is different from previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave:  1\n",
      "X_personal:  (4019, 103)\n",
      "X_political:  (4019, 354)\n",
      "Wave:  2\n",
      "X_personal:  (3133, 103)\n",
      "X_political:  (3133, 335)\n",
      "Wave:  3\n",
      "X_personal:  (2994, 103)\n",
      "X_political:  (2994, 452)\n",
      "Wave:  4\n",
      "X_personal:  (3166, 103)\n",
      "X_political:  (3166, 622)\n",
      "Wave:  5\n",
      "X_personal:  (3026, 103)\n",
      "X_political:  (3026, 351)\n",
      "Wave:  6\n",
      "X_personal:  (2974, 101)\n",
      "X_political:  (2974, 372)\n"
     ]
    }
   ],
   "source": [
    "WAVENOS = ['1', '2', '3', '4', '5', '6']\n",
    "for wave in WAVENOS:\n",
    "    print('Wave: ', wave)\n",
    "    print('X_personal: ', X_personal[wave].shape)\n",
    "    print('X_political: ', X_political[wave].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for NaN in all datasets and getting its index if relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave:  1\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n",
      "Wave:  2\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n",
      "Wave:  3\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n",
      "Wave:  4\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n",
      "Wave:  5\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n",
      "Wave:  6\n",
      "False\n",
      "[]\n",
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for wave in WAVENOS:\n",
    "    print('Wave: ', wave)\n",
    "    print(X_personal[wave].isnull().values.any())\n",
    "    print(X_personal[wave].loc[pd.isnull(\n",
    "        X_personal[wave]).any(1), :].index.values)\n",
    "    print(X_political[wave].isnull().values.any())\n",
    "    print(X_political[wave].loc[pd.isnull(\n",
    "        X_political[wave]).any(1), :].index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wave in WAVENOS:\n",
    "    X_political[wave].to_csv(\n",
    "        f'data/data_online_political_w{wave}.csv', index=False)\n",
    "    X_personal[wave].to_csv(\n",
    "        f'data/data_online_personal_w{wave}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
